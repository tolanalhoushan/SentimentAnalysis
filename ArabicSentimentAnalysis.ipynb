{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f63bc9fc-4a0f-42c1-8378-69e1a275b438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./tf-env/lib/python3.10/site-packages (2.3.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in ./tf-env/lib/python3.10/site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./tf-env/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./tf-env/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./tf-env/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./tf-env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2025.9.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m36m-:--:--\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.9.1-cp310-cp310-macosx_11_0_arm64.whl (286 kB)\n",
      "Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5/5\u001b[0m [nltk][32m4/5\u001b[0m [nltk]b]\n",
      "\u001b[1A\u001b[2KSuccessfully installed click-8.2.1 joblib-1.5.2 nltk-3.9.1 regex-2025.9.1 tqdm-4.67.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a990876e-67f6-4a37-9e2d-d3aedb8db11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.1-cp310-cp310-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.22.0 in ./tf-env/lib/python3.10/site-packages (from scikit-learn) (2.2.6)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Downloading scipy-1.15.3-cp310-cp310-macosx_14_0_arm64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./tf-env/lib/python3.10/site-packages (from scikit-learn) (1.5.2)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.7.1-cp310-cp310-macosx_12_0_arm64.whl (8.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.15.3-cp310-cp310-macosx_14_0_arm64.whl (22.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m22.4/22.4 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, scikit-learn\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3/3\u001b[0m [scikit-learn][0m [scikit-learn]\n",
      "\u001b[1A\u001b[2KSuccessfully installed scikit-learn-1.7.1 scipy-1.15.3 threadpoolctl-3.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2288d0df-ae6e-4a86-bcd3-8c6c14a4831f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding,LSTM,Dense\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d28a2e7-0fa0-415e-951e-a818a6e94956",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/tolan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /Users/tolan/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "38b4d2cb-3a2b-4a56-9380-2f1ff55b5d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ù„ÙŠÙ„Ù‡ Ø³ØªÙƒØªØ¨ Ø¹Ù†Ù‡Ø§ ÙƒÙ„ Ø§Ù„Ø£Ù‚Ù„Ø§Ù… Ù„ÙŠÙ„Ù‡ Ø³ØªØ­ØµØ¯ ÙÙŠ Ø§Ù„ØªØ§Ø±...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ø§Ù„Ù‰ Ù…ØªÙ‰ Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ø§Ù„Ø³Ø¦ Ù„Ù„Ø®Ø§Ø¯Ù…Ø§Øª ÙˆØ¹Ø¯Ù… Ø§Ø­ØªØ±Ø§Ù…Ù‡Ù… ÙˆÙƒ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ù‡Ùˆ Ø§ÙƒÙŠØ¯ Ø´Ø§ÙŠÙ ÙˆÙ…ØªØ°ÙƒØ± Ø§Ù„Ø§ØºÙ„Ø¨ ğŸ¤” Ø¨Ø³ Ù„Ùˆ Ø¨Ù…Ù†Ø´Ù† ÙˆÙŠØ±Ø¯ ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ù…Ø§Ø¹Ù†Ø¯Ù†Ø§ Ø´Ùƒ ÙÙŠÙƒ ÙŠØ§Ù…Ø¹Ù„Ù… ÙˆÙ†Ø¹Ù„Ù… Ù…Ø§Ø¨Ø¯Ø§Ø®Ù„Ùƒ Ù…Ù† Ø¹Ø´Ù‚ Ù„Ù„...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ø±Ø¦ÙŠØ³ Ø§Ù„Ù…Ø¬Ù„Ø³ Ø§Ù„Ø¹Ø³ÙƒØ±ÙŠ Ø§Ù„Ø³ÙˆØ¯Ø§Ù†ÙŠ Ø§Ù„ÙØ±ÙŠÙ‚ Ø§Ù„Ø±ÙƒÙ† #Ø¹ÙˆØ¶...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ÙÙŠ Ù…Ø¨Ø§Ø±Ø§Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ØºÙˆÙ…ÙŠØ² Ø§Ø³ØªÙØ² ÙˆØªÙ„ÙØ¸ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¯Ø§Ù...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ø§Ø®ØªÙŠ Ø±Ø§Ø­Øª Ù„Ù„Ø´Ø§Ø·ÙŠ ÙˆØ§ÙØ·Ø±Øª ÙˆØªÙ‚Ù‡ÙˆØª Ø¨Ù‡Ø°Ø§ Ø§Ù„Ø¬Ùˆ Ø­Ù„Ùˆ Ùˆ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ø§Ù„Ù„Ù‡ ÙŠØ´ÙÙŠÙƒ ÙˆÙŠØ±ÙŠØ­Ù†Ø§ Ù…Ù† Ø§Ù…Ø«Ø§Ù„Ùƒ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ÙˆÙ…Ù† Ø£Ø­ÙŠØ§Ù‡Ø§ ÙÙƒØ£Ù†Ù…Ø§ Ø£Ø­ÙŠØ§ Ø§Ù„Ù†Ø§Ø³ Ø¬Ù…ÙŠØ¹Ø§ Ø´Ø§Ø¨Ø© (Ù¡Ù¨Ø³Ù†Ø©...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ø¬Ù†Ø§Ø¨Ùˆ Ø²Ø§ØªÙˆ Ù…Ø­ØªØ§Ø± \" Ø§ØªÙ†ÙŠÙ† ÙÙŠ ÙŠÙˆÙ…ÙŠÙ† ÙŠØ¬Ù…Ø§Ø¹Ù‡\" ğŸ˜‚</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Ù„ÙŠÙ„Ù‡ Ø³ØªÙƒØªØ¨ Ø¹Ù†Ù‡Ø§ ÙƒÙ„ Ø§Ù„Ø£Ù‚Ù„Ø§Ù… Ù„ÙŠÙ„Ù‡ Ø³ØªØ­ØµØ¯ ÙÙŠ Ø§Ù„ØªØ§Ø±...      0\n",
       "1  Ø§Ù„Ù‰ Ù…ØªÙ‰ Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ø§Ù„Ø³Ø¦ Ù„Ù„Ø®Ø§Ø¯Ù…Ø§Øª ÙˆØ¹Ø¯Ù… Ø§Ø­ØªØ±Ø§Ù…Ù‡Ù… ÙˆÙƒ...      0\n",
       "2  Ù‡Ùˆ Ø§ÙƒÙŠØ¯ Ø´Ø§ÙŠÙ ÙˆÙ…ØªØ°ÙƒØ± Ø§Ù„Ø§ØºÙ„Ø¨ ğŸ¤” Ø¨Ø³ Ù„Ùˆ Ø¨Ù…Ù†Ø´Ù† ÙˆÙŠØ±Ø¯ ...      1\n",
       "3  Ù…Ø§Ø¹Ù†Ø¯Ù†Ø§ Ø´Ùƒ ÙÙŠÙƒ ÙŠØ§Ù…Ø¹Ù„Ù… ÙˆÙ†Ø¹Ù„Ù… Ù…Ø§Ø¨Ø¯Ø§Ø®Ù„Ùƒ Ù…Ù† Ø¹Ø´Ù‚ Ù„Ù„...      1\n",
       "4  Ø±Ø¦ÙŠØ³ Ø§Ù„Ù…Ø¬Ù„Ø³ Ø§Ù„Ø¹Ø³ÙƒØ±ÙŠ Ø§Ù„Ø³ÙˆØ¯Ø§Ù†ÙŠ Ø§Ù„ÙØ±ÙŠÙ‚ Ø§Ù„Ø±ÙƒÙ† #Ø¹ÙˆØ¶...      1\n",
       "5  ÙÙŠ Ù…Ø¨Ø§Ø±Ø§Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ØºÙˆÙ…ÙŠØ² Ø§Ø³ØªÙØ² ÙˆØªÙ„ÙØ¸ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¯Ø§Ù...      1\n",
       "6  Ø§Ø®ØªÙŠ Ø±Ø§Ø­Øª Ù„Ù„Ø´Ø§Ø·ÙŠ ÙˆØ§ÙØ·Ø±Øª ÙˆØªÙ‚Ù‡ÙˆØª Ø¨Ù‡Ø°Ø§ Ø§Ù„Ø¬Ùˆ Ø­Ù„Ùˆ Ùˆ...      0\n",
       "7                       Ø§Ù„Ù„Ù‡ ÙŠØ´ÙÙŠÙƒ ÙˆÙŠØ±ÙŠØ­Ù†Ø§ Ù…Ù† Ø§Ù…Ø«Ø§Ù„Ùƒ      0\n",
       "8  ÙˆÙ…Ù† Ø£Ø­ÙŠØ§Ù‡Ø§ ÙÙƒØ£Ù†Ù…Ø§ Ø£Ø­ÙŠØ§ Ø§Ù„Ù†Ø§Ø³ Ø¬Ù…ÙŠØ¹Ø§ Ø´Ø§Ø¨Ø© (Ù¡Ù¨Ø³Ù†Ø©...      1\n",
       "9        Ø¬Ù†Ø§Ø¨Ùˆ Ø²Ø§ØªÙˆ Ù…Ø­ØªØ§Ø± \" Ø§ØªÙ†ÙŠÙ† ÙÙŠ ÙŠÙˆÙ…ÙŠÙ† ÙŠØ¬Ù…Ø§Ø¹Ù‡\" ğŸ˜‚      1"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "neg_df = pd.read_csv(\"test_Arabic_tweets_negative_20190413.csv\", header=None, names=[\"text\"])\n",
    "neg_df['label'] = 0\n",
    "\n",
    "pos_df = pd.read_csv(\"test_Arabic_tweets_positive_20190413.csv\", header=None, names=[\"text\"])\n",
    "pos_df['label'] = 1\n",
    "df = pd.concat([neg_df, pos_df], ignore_index=True)\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a266ccbe-8348-4eaf-ae7c-6a6fdc862070",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing text\n",
    "\n",
    "stop_words = set(stopwords.words('arabic'))\n",
    "texts = df['text'].astype(str).tolist()\n",
    "labels = df['label'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2266fbf8-c32f-43fb-9992-524c54be010b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000  # max vocabulary size\n",
    "input_length = 100    # max length of sequences\n",
    "embedding_dim = 100\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "fbd836b7-efb8-4846-8125-38fa38aea4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "aedd6ff6-0389-487d-83b6-3c3bb6ec4061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9216, 100)\n",
      "False\n",
      "[0 0 0 1 1 1 0 0 0 0]\n",
      "int32\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(X_train.shape)\n",
    "print(np.isnan(X_train).any())\n",
    "print(y_train[:10])\n",
    "print(X_train.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3701274f-0de0-49bc-8f2f-4f0381ec1699",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unrecognized keyword arguments passed to Embedding: {'input_len': 100}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[113], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model\u001b[38;5;241m=\u001b[39mSequential()\n\u001b[0;32m----> 2\u001b[0m model\u001b[38;5;241m.\u001b[39madd(\u001b[43mEmbedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43moutput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43minput_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_length\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39madd(LSTM(\u001b[38;5;241m128\u001b[39m))\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(\u001b[38;5;241m1\u001b[39m,activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m~/tf-env/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:100\u001b[0m, in \u001b[0;36mEmbedding.__init__\u001b[0;34m(self, input_dim, output_dim, embeddings_initializer, embeddings_regularizer, embeddings_constraint, mask_zero, weights, lora_rank, lora_alpha, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     97\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `input_length` is deprecated. Just remove it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     99\u001b[0m     )\n\u001b[0;32m--> 100\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_dim \u001b[38;5;241m=\u001b[39m input_dim\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dim \u001b[38;5;241m=\u001b[39m output_dim\n",
      "File \u001b[0;32m~/tf-env/lib/python3.10/site-packages/keras/src/layers/layer.py:291\u001b[0m, in \u001b[0;36mLayer.__init__\u001b[0;34m(self, activity_regularizer, trainable, dtype, autocast, name, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_shape_arg \u001b[38;5;241m=\u001b[39m input_shape_arg\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    292\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized keyword arguments \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    293\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassed to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    294\u001b[0m     )\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# Will be determined in `build_wrapper`\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Unrecognized keyword arguments passed to Embedding: {'input_len': 100}"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size,output_dim=embedding_dim,input_len=input_length))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.build(input_shape=(None,input_len))\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b8ae8151-13d1-4c44-9652-6a1a7857287e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m710/710\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1897s\u001b[0m 3s/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 2/5\n",
      "\u001b[1m710/710\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 103ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 3/5\n",
      "\u001b[1m710/710\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 101ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 4/5\n",
      "\u001b[1m710/710\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 95ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 5/5\n",
      "\u001b[1m710/710\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 100ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x173d41d20>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "bd4af0b6-2e03-45bb-9608-e62d3d43450d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.4770 - loss: 0.6933\n",
      "test Accuracy:0.48%\n"
     ]
    }
   ],
   "source": [
    "loss,accuracy =model.evaluate(X_test,y_test)\n",
    "print(f\"test Accuracy:{accuracy:.2f}%\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "87462ce5-9f2d-4b0c-ba78-e6b97f76681b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    clean_text=preprocess_text(text)\n",
    "    encoded_text=tokenizer.texts_to_sequences([clean_text])\n",
    "    padded_text=pad_sequences(encoded_text,maxlen=max_length,padding=\"post\")\n",
    "    sentiment=model.predict(padded_text)\n",
    "    return sentiment\n",
    "    text=\"Ø§Ù…Ø·Ø±Øª Ø¨Ø§Ù„Ù„ÙŠÙ„ ÙˆØ§Ù†Ø§ Ù†Ø§ÙŠÙ…Ù‡ØŸ ğŸ˜”\"\n",
    "    sentiment=predict_sentiment(text)\n",
    "    print(\"Sentiment: {:.2f}\".format(sentiment[0][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123c029b-2fde-4406-8a74-925098ba6668",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (TensorFlow)",
   "language": "python",
   "name": "tf-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
